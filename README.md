# AwesomeGAIManipulation

## Survey

## Data Generation
- **GRUtopia: Dream General Robots in a City at Scale**
  [[paper]](https://arxiv.org/abs/2407.10943)
  [[code]](https://github.com/OpenRobotLab/GRUtopia)
- **Diffusion for Multi-Embodiment Grasping**
  [[paper]](https://arxiv.org/html/2410.18835v1)
- **Gen2sim: Scaling up robot learning in simulation with generative model (ICRA 2024)**
  [[paper]](https://arxiv.org/abs/2310.18308)
  [[code]](https://github.com/pushkalkatara/Gen2Sim)
  [[webpage]](https://gen2sim.github.io/)
- **RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation (ICML 2024)**
  [[paper]](https://arxiv.org/abs/2311.01455)
  [[code]](https://github.com/Genesis-Embodied-AI/RoboGen)
  [[webpage]](https://robogen-ai.github.io/)
- **Holodeck: Language Guided Generation of 3D Embodied AI Environments (CVPR 2024)**
  [[paper]](https://arxiv.org/abs/2312.09067)
  [[code]](https://github.com/allenai/Holodeck)
  [[webpage]](https://yueyang1996.github.io/holodeck/)
- **Video Generation Models as World Simulators**
  [[paper]](https://arxiv.org/abs/2410.18072)
  [[webpage]](https://openai.com/research/video-generation-models-as-world-simulators)
- **Learning Interactive Real-World Simulators**
  [[paper]](https://arxiv.org/abs/2310.06114)
- **MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations (CoRL 2023)**
  [[paper]](https://proceedings.mlr.press/v229/mandlekar23a/mandlekar23a.pdf)
  [[code]](https://mimicgen.github.io)
  [[webpage]](https://mimicgen.github.io/)
- **CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation (CVPR 2024)**
  [[paper]](https://arxiv.org/pdf/2402.14795)
  [[code]](https://github.com/wang59695487/hand_teleop_real_sim_mix_adr)
  [[webpage]]( https://cyber-demo.github.io/)
- **Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning**
  [[paper]](https://arxiv.org/abs/2402.17768)
  [[code]](https://github.com/ErinZhang1998/dmd_diffusion)
  [[webpage]]()
- **DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning (ICRA 2025)**
  [[paper]](https://arxiv.org/pdf/2410.24185)
  [[code]](https://github.com/NVlabs/dexmimicgen/)
  [[webpage]](https://dexmimicgen.github.io/)
- **IntervenGen: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning**
  [[paper]](https://arxiv.org/abs/2405.01472)
  [[webpage]](https://sites.google.com/view/intervengen2024)
- **Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition (CoRL 2023)**
  [[paper]](https://proceedings.mlr.press/v229/ha23a/ha23a.pdf)
  [[code]](https://github.com/real-stanford/scalingup)
  [[webpage]](https://www.cs.columbia.edu/~huy/scalingup/)
- **GenAug: Retargeting behaviors to unseen situations via Generative Augmentation (RSS 2023)**
  [[paper]](https://arxiv.org/abs/2302.06671)
  [[code]](https://github.com/genaug/genaug)
  [[webpage]](https://genaug.github.io/)
- **Scaling Robot Learning with Semantically Imagined Experience (RSS 2023)**
  [[paper]](https://arxiv.org/abs/2302.11550)
  [[webpage]](https://diffusion-rosie.github.io/)
- **RoVi-Aug: Robot and Viewpoint Augmentation for Cross-Embodiment Robot Learning (CoRL 2024)**
  [[paper]](https://rovi-aug.github.io/static/pdf/rovi_aug_paper.pdf)
  [[code]](https://github.com/BerkeleyAutomation/rovi-aug)
  [[webpage]](https://rovi-aug.github.io/)
- **Learning Robust Real-World Dexterous Grasping Policies via Implicit Shape Augmentation (CoRL 2022)**
  [[paper]](https://arxiv.org/abs/2210.13638)
  [[webpage]]( https://sites.google.com/view/implicitaugmentation/home)
- **DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics**
  [[paper]](https://arxiv.org/abs/2210.02438)
- **Shadow: Leveraging Segmentation Masks for Cross-Embodiment Policy Transfer (CoRL 2024)**
  [[paper]](https://shadow-cross-embodiment.github.io/static/shadow24.pdf)
  [[code]](https://shadow-cross-embodiment.github.io/)
- **Human-to-Robot Imitation in the Wild (RSS 2022)**
  [[paper]](https://arxiv.org/abs/2207.09450)
  [[webpage]](https://human2robot.github.io/)
- **Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting (RSS 2024)**
  [[paper]]( https://robot-mirage.github.io/static/pdf/mirage_paper.pdf)
  [[code]](https://github.com/BerkeleyAutomation/mirage)
  [[webpage]](https://robot-mirage.github.io/)
- **CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning**
  [[paper]](https://arxiv.org/abs/2212.05711)
  [[webpage]](https://cacti-framework.github.io/)
- **RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking (ICRA 2024)**
  [[paper]](https://arxiv.org/abs/2309.01918)
  [[code]](https://github.com/robopen/roboagent/)
  [[webpage]](https://robopen.github.io/)
- **ExAug: Robot-Conditioned Navigation Policies via Geometric Experience Augmentation (ICRA 2023)**
  [[paper]](https://arxiv.org/abs/2210.07450)
  [[code]](https://github.com/NHirose/ExAug)
  [[webpage]](https://sites.google.com/view/exaug-nav)
- **RACER: Rich Language-Guided Failure Recovery Policies for Imitation Learning (ICRA 2025)**
  [[paper]](https://arxiv.org/abs/2409.14674)
  [[code]](https://github.com/sled-group/RACER)
  [[webpage]]( https://rich-language-failure-recovery.github.io/)
- **Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models (RSS 2023)**
  [[paper]](https://arxiv.org/abs/2211.11736)
  [[webpage]](https://instructionaugmentation.github.io/)

      
## Reward Generation
- **Language to Rewards for Robotic Skill Synthesis (CoRL 2023)**
  [[paper]](https://openreview.net/forum?id=SgTPdyehXMA)
  [[code]](https://github.com/google-deepmind/language_to_reward_2023)
  [[webpage]](https://language-to-reward.github.io/)
- **Vision-Language Models as Success Detectors (CoLLA 2023)**
  [[paper]](https://proceedings.mlr.press/v232/du23b/du23b.pdf)
- **Scaling robot policy learning via zero-shot labeling with foundation models (CoRL 2024)**
  [[paper]](https://arxiv.org/abs/2410.17772)
  [[code]](https://robottasklabeling.github.io/)
  [[webpage]](https://robottasklabeling.github.io/) 
- **FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning (ICML 2024)**
  [[paper]]([https://arxiv.org/abs/2402.00000](https://arxiv.org/abs/2406.00645))
  [[code]](https://github.com/fuyw/FuRL)  
- **Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning (ICLR 2024)**
  [[paper]](https://openreview.net/forum?id=tUM39YTRxH)
- **Eureka: Human-Level Reward Design via Coding Large Language Models (NeurIPS 2023)**
  [[paper]](https://arxiv.org/abs/2310.12931)
- **Agentic Skill Discovery (CoRL 2024 workshop & ICRA@40)**
  [[paper]](https://arxiv.org/abs/2405.15019)
  [[code]](https://github.com/xf-zhao/Agentic-Skill-Discovery)
- **CLIPort: What and Where Pathways for Robotic Manipulation**
  [[paper]](https://arxiv.org/abs/2109.12098)
- **R3M: A Universal Visual Representation for Robot Manipulation**
  [[paper]](https://arxiv.org/abs/2203.12601)
  [[code]](https://github.com/facebookresearch/r3m)
  [[webpage]](https://sites.google.com/view/robot-r3m/?pli=1)
- **LIV: Language-Image Representations and Rewards for Robotic Control (ICML 2023)**
  [[paper]](https://arxiv.org/abs/2306.00958)
  [[code]](https://github.com/penn-pal-lab/LIV)
  [[webpage]](https://penn-pal-lab.github.io/LIV/)
- **Learning Reward Functions for Robotic Manipulation by Observing Humans**
  [[paper]](https://arxiv.org/abs/2211.09019)
- **Deep visual foresight for planning robot motion (ICRA 2017)**
  [[paper]](https://arxiv.org/abs/1610.00696)
- **VLMPC: Vision-Language Model Predictive Control for Robotic Manipulation (RSS 2024)**
  [[paper]](https://arxiv.org/abs/2407.09829)
  [[code]](https://github.com/PPjmchen/VLMPC)
- **Learning Reward for Robot Skills Using Large Language Models via Self-Alignment (ICML 2024)**
  [[paper]](https://arxiv.org/abs/2405.07162)
- **Video Prediction Models as Rewards for Reinforcement Learning**
  [[paper]](https://arxiv.org/abs/2305.14343)
  [[code]](https://escontrela.me/viper)
- **Vip: Towards universal visual reward and representation via value-implicit pre-training (ICLR 2023)**
  [[paper]](https://arxiv.org/abs/2210.00030)
  [[code]](https://github.com/facebookresearch/vip)
- **Learning to Understand Goal Specifications by Modelling Reward**
  [[paper]](https://arxiv.org/pdf/1806.01946)
- **Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks**
  [[paper]](https://arxiv.org/abs/2405.01534)
- **Policy improvement using language feedback models (NeurIPS 2024)**
  [[paper]](https://arxiv.org/abs/2402.07876)



## State Generation


- **Reinforcement learning with action-free pre-training from videos (ICML2022)**
  [[paper]](https://proceedings.mlr.press/v162/seo22a/seo22a.pdf)
  [[code]](https://github.com/younggyoseo/apv)
- **Mastering diverse domains through world models**
  [[paper]](https://arxiv.org/pdf/2301.04104v2)
  [[code]](https://github.com/danijar/dreamerv3)
  [[webpage]](https://danijar.com/project/dreamerv3/)
- **Dream to Control: Learning Behaviors by Latent Imagination**
  [[paper]](https://arxiv.org/abs/1912.01603)
- **Robot Shape and Location Retention in Video Generation Using Diffusion Models**
  [[paper]](https://arxiv.org/abs/2407.02873)
  [[code]](https://github.com/PengPaulWang/diffusion-robots)
- **Uncertainty-aware active learning of nerf-based object models for robot manipulators using visual and re-orientation actions**
  [[paper]](https://actnerf.github.io/)
  [[code]](https://github.com/ActNeRF/ActNeRF)
  [[webpage]](https://actnerf.github.io/)
- **Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors**
  [[paper]](https://arxiv.org/abs/2403.14526)
  [[code]](https://github.com/tsagkas/click2grasp)
  [[webpage]](https://tsagkas.github.io/click2grasp/)
- **Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL (ECCV2024)**
  [[paper]](https://arxiv.org/abs/2404.09857)
- **Doughnet: A visual predictive model for topological manipulation of deformable objects**
  [[paper]](https://arxiv.org/abs/2404.12524)
- **KISA: A Unified Keyframe Identifier and Skill Annotator for Long-Horizon Robotics Demonstrations (ICML2024)**
  [[paper]](https://openreview.net/pdf?id=oCI9gHocws)
- **DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems (ICML2024)**
  [[paper]](https://arxiv.org/abs/2407.11472)
- **Symmetry-Aware Robot Design with Structured Subgroups (ICML2023)**
  [[paper]](https://arxiv.org/abs/2306.00036)
- **Total-recon: Deformable scene reconstruction for embodied view synthesis (ICCV2023)**
  [[paper]](https://arxiv.org/abs/2304.12317)
  [[code & data]](https://github.com/andrewsonga/Total-Recon)
  [[webpage]](https://andrewsonga.github.io/totalrecon)
- **Explore and Tell: Embodied Visual Captioning in 3D Environments (ICCV2023)**
  [[paper]](https://arxiv.org/abs/2308.10447)
  [[code & data]](https://aim3-ruc.github.io/ExploreAndTell)
- **Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation (ECCV2024)**
  [[paper]](https://arxiv.org/abs/2405.01527)
- **Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation (CoRL2024)**
  [[paper]](https://arxiv.org/abs/2403.08321)
  [[code]](https://github.com/GuanxingLu/ManiGaussian)
  [[webpage]](https://guanxinglu.github.io/ManiGaussian/)
- **Physically Embodied Gaussian Splatting: A Realtime Correctable World Model for Robotics**
  [[paper]](https://arxiv.org/abs/2406.10788)
- **Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training (NeurIPS2024)**
  [[paper]](https://arxiv.org/pdf/2402.14407)
  [[code]](https://github.com/tinnerhrhe/VPDD)
  [[webpage]](https://video-diff.github.io/)
- **PreLAR: World Model Pre-training with Learnable Action Representation (ECCV2024)**
  [[paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03363.pdf)
  [[code]](https://github.com/zhanglixuan0720/PreLAR)
- **Octopus: Embodied vision-language programmer from environmental feedback**
  [[paper]](https://arxiv.org/abs/2310.08588)
  [[code]](https://github.com/dongyh20/Octopus)
  [[webpage]](https://choiszt.github.io/Octopus/)
- **Ec2: Emergent communication for embodied control (CVPR2023)**
  [[papar]](https://arxiv.org/abs/2304.09448)
- **Voxposer: Composable 3d value maps for robotic manipulation with language models**
  [[paper]](https://arxiv.org/abs/2307.05973)

## Language Generation
- **Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (PMLR 2022)**
  [[paper]](https://arxiv.org/pdf/2201.07207.pdf)
  [[code]](https://github.com/huangwl18/language-planner)
- **Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition (PMLR 2023)**
  [[paper]](https://arxiv.org/abs/2307.14535)
  [[code]](https://github.com/real-stanford/scalingup)
- **Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks (ICLR 2024)**
  [[paper]](https://arxiv.org/pdf/2405.01534)
  [[code]](https://github.com/mihdalal/planseqlearn)
- **Large language models as commonsense knowledge for large-scale task planning (NeurIPS 2023)**
  [[paper]](https://arxiv.org/abs/2305.14078)
  [[code]](https://github.com/1989Ryan/llm-mcts)
- **REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction (CoRL 2023)**
  [[paper]](https://arxiv.org/abs/2306.15724)
  [[code]](https://github.com/real-stanford/reflect)
- **Gesture-Informed Robot Assistance via Foundation Models (CoRL 2023)**
  [[paper]](https://openreview.net/pdf?id=Ffn8Z4Q-zU)
- **Large Language Models for Robotics: Opportunities, Challenges, and Perspectives**
  [[paper]](https://arxiv.org/pdf/2401.04334)
- **Embodied Agent Interface (EAI): Benchmarking LLMs for Embodied Decision Making (NeurIPS 2024 Track Datasets and Benchmarks)**
  [[paper]](https://arxiv.org/abs/2410.07166)
  [[code]](https://github.com/embodied-agent-interface/embodied-agent-interface)
- **Embodiedgpt: Vision-language pre-training via embodied chain of thought (NeurIPS 2023)**
  [[paper]](https://arxiv.org/pdf/2305.15021.pdf)
  [[code]](https://github.com/OpenGVLab/EmbodiedGPT)
- **Chat with the Environment: Interactive Multimodal Perception using Large Language Models (IROS 2023)**
  [[paper]](https://arxiv.org/abs/2303.08268)
  [[code]](https://github.com/xf-zhao/Matcha)
- **Embodied CoT Distillation From LLM To Off-the-shelf Agents (ICML 2024)**
   [[paper]](https://arxiv.org/html/2412.11499v1)
- **Do as i can, not as i say: Grounding language in robotic affordances**
  [[paper]](https://say-can.github.io/assets/palm_saycan.pdf)
  [[code]](https://github.com/google-research/google-research/tree/master/saycan)
- **Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents (NeurIPS 2023)**
  [[paper]](https://openreview.net/pdf?id=JCCi58IUsh)
- **Inner Monologue: Embodied Reasoning through Planning with Language Models (CoRL 2022)**
 [[paper]](https://arxiv.org/abs/2207.05608)
- **PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large Multimodal Models**
  [[paper]](https://arxiv.org/pdf/2402.16836.pdf)
  [[code]](https://github.com/dkguo/PhyGrasp)
- **SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning (CoRL 2023)**
  [[paper]](https://arxiv.org/abs/2307.06135)
- **Robomp2: A robotic multimodal perception-planning framework with multimodal large language models (ICML 2024)**
  [[paper]](https://arxiv.org/abs/2404.04929)
  [[code]](https://github.com/aopolin-lv/RoboMP2)
- **Text2Motion: From Natural Language Instructions to Feasible Plans (Autonomous Robots 2023)**
   [[paper]](https://openreview.net/pdf?id=M1yTyG5P7Cl)
- **STAP: Sequencing Task-Agnostic Policies (ICRA 2023)**
  [[paper]](https://arxiv.org/abs/2210.12250)
  [[code]](https://github.com/agiachris/STAP)
  
## Code Generation

- **Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V (arXiv 2024)**
  [[paper]](https://arxiv.org/abs/2404.10220)
- **ProgPrompt: Program Generation for Situated Robot Task Planning Using Large Language Models (Autonomous Robots 2023)**
  [[paper]](https://arxiv.org/abs/2209.11302)
- **See and Think: Embodied Agent in Virtual Environment (arXiv 2023)**
  [[paper]](https://arxiv.org/abs/2311.15209)
- **Octopus: Embodied Vision-Language Programmer from Environmental Feedback (ECCV 2024)**
  [[paper]](https://arxiv.org/abs/2310.08588)
  [[webpage]](https://choiszt.github.io/Octopus/)
  [[code]](https://github.com/dongyh20/Octopus)
- **Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought (NeurIPS 2023)**
  [[paper]](https://arxiv.org/abs/2305.16744)
  [[webpage]](https://portal-cornell.github.io/demo2code/)
  [[code]](https://github.com/portal-cornell/demo2code)
- **EC2: Emergent Communication for Embodied Control (CVPR 2023)**
  [[paper]](https://arxiv.org/abs/2304.09448)
- **When Prolog Meets Generative Models: A New Approach for Managing Knowledge and Planning in Robotic Applications (ICRA 2024)**
  [[paper]](https://arxiv.org/abs/2309.15049)
- **Code as Policies: Language Model Programs for Embodied Control (ICRA 2023)**
  [[paper]](https://arxiv.org/abs/2209.07753)
  [[webpage]](https://code-as-policies.github.io/)
  [[code]](https://github.com/google-research/google-research/tree/master/code_as_policies)
- **GenCHiP: Generating Robot Policy Code for High-Precision and Contact-Rich Manipulation Tasks (arXiv 2024)**
  [[paper]](https://arxiv.org/abs/2404.06645)
- **VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models (CoRL 2023)**
  [[paper]](https://arxiv.org/abs/2307.05973)
  [[webpage]](https://voxposer.github.io/)
  [[code]](https://github.com/huangwl18/VoxPoser)
- **ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation (arXiv 2024)**
  [[paper]](https://arxiv.org/abs/2409.01652)
  [[webpage]](https://rekep-robot.github.io/)
  [[code]](https://github.com/huangwl18/ReKep)
- **RoboScript: Code Generation for Free-Form Manipulation Tasks Across Real and Simulation (arXiv 2024)**
  [[paper]](https://arxiv.org/abs/2402.16117)
- **RobotGPT: Robot Manipulation Learning From ChatGPT (RAL 2024)**
  [[paper]](https://arxiv.org/abs/2312.01421)
- **RoboCodeX: Multimodal Code Generation for Robotic Behavior Synthesis (ICML 2024)**
  [[paper]](https://arxiv.org/abs/2402.16117)
  [[webpage]](https://sites.google.com/view/robocodex)
  [[code]](https://github.com/RoboCodeX-source/RoboCodeX_code)
- **Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model (arXiv 2023)**
  [[paper]](https://arxiv.org/abs/2305.11176)
  [[code]](https://github.com/OpenGVLab/Instruct2Act)
- **GenSim: Generating Robotic Simulation Tasks via Large Language Models (ICLR 2024)**
  [[paper]](https://arxiv.org/abs/2310.01361)
  [[code]](https://github.com/liruiw/GenSim)

## Visual Generation
- **Learning Universal Policies via Text-Guided Video Generation (NeurIPS 2023)**
  [[paper]](https://arxiv.org/abs/2302.00111)
  [[webpage]](https://universal-policy.github.io/unipi/)
- **SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation (ICLR 2025)**
  [[paper]](https://arxiv.org/abs/2410.23277)
  [[webpage]](https://slowfast-vgen.github.io/)
- **Using Left and Right Brains Together: Towards Vision and Language Planning (ICML 2024)**
  [[paper]](https://arxiv.org/abs/2402.10534)
- **Compositional Foundation Models for Hierarchical Planning (NeurIPS 2023)**
  [[paper]](https://arxiv.org/abs/2309.08587)
  [[webpage]](https://hierarchical-planning-foundation-model.github.io/)
- **Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation (NeurIPS 2024)**
  [[paper]](https://arxiv.org/abs/2409.09016)
  [[code]](https://github.com/OpenDriveLab/CLOVER)
- **GR-1: Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation**
  [[webpage]](https://gr1-manipulation.github.io)
  [[code]](https://github.com/bytedance/GR-1)
- **GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation**
  [[webpage]](https://gr2-manipulation.github.io)
- **Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models (ICLR 2024)**
  [[paper]](https://arxiv.org/abs/2310.10639)
  [[webpage]](https://rail-berkeley.github.io/susie/)
  [[code]](https://github.com/kvablack/susie)
- **Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts (CVPR 2024)**
  [[paper]](https://openaccess.thecvf.com/content/CVPR2024/html/Ni_Generate_Subgoal_Images_before_Act_Unlocking_the_Chain-of-Thought_Reasoning_in_CVPR_2024_paper.html)
  [[webpage]](https://cotdiffusion.github.io/)
- **Surfer: Progressive Reasoning with World Models for Robotic Manipulation**
  [[paper]](https://arxiv.org/abs/2306.11335)
- **TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation (CoRL 2022)**
  [[paper]](https://arxiv.org/abs/2211.09325)
  [[webpage]](https://sites.google.com/view/tax-pose/home)
  [[code]](https://github.com/r-pad/taxpose)
- **Imagination Policy: Using Generative Point Cloud Models for Learning Manipulation Policies (CoRL 2024)**
  [[paper]](https://arxiv.org/abs/2406.11740)
  [[webpage]](https://haojhuang.github.io/imagine_page/)
  [[code]](https://github.com/HaojHuang/imagination-policy-cor24)
- **Uncertainty-aware Active Learning of NeRF-based Object Models for Robot Manipulators using Visual and Re-orientation Actions**
  [[paper]](https://arxiv.org/abs/2404.01812)
  [[webpage]](https://actnerf.github.io/)
  [[code]](https://github.com/ActNeRF/ActNeRF)
- **Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation (CoRL 2022)**
  [[paper]](https://arxiv.org/abs/2209.05451)
  [[webpage]](https://peract.github.io/)
  [[code]](https://github.com/peract/peract)
- **ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation (ECCV 2024)**
  [[paper]](https://arxiv.org/abs/2403.08321)
  [[webpage]](https://guanxinglu.github.io/ManiGaussian/)
  [[code]](https://github.com/GuanxingLu/ManiGaussian)
- **GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields (CoRL 2023)**
  [[paper]](https://arxiv.org/abs/2308.16891)
  [[webpage]](https://yanjieze.com/GNFactor/)
  [[code]](https://github.com/YanjieZe/GNFactor)
- **WorldVLA: Towards Autoregressive Action World Model**
  [[paper]](https://arxiv.org/pdf/2506.21539)
  [[webpage]](https://github.com/alibaba-damo-academy/WorldVLA)
  [[code]](https://github.com/alibaba-damo-academy/WorldVLA)




## Grasp Generation

## Trajectory Generation
- **Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation**
  [[webpage]](https://mobile-aloha.github.io)
- **Diffusion Policy: Visuomotor Policy Learning via Action Diffusion**
  [[webpage]](https://diffusion-policy.cs.columbia.edu)
- **3D Diffuser Actor: Policy Diffusion with 3D Scene Representations**
  [[webpage]](https://3d-diffuser-actor.github.io)
- **RT-1: Robotics Transformer for Real-World Control at Scale**
  [[webpage]](https://robotics-transformer1.github.io)
- **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control**
  [[webpage]](https://robotics-transformer2.github.io)
- **RVT: Robotic View Transformer for 3D Object Manipulation**
  [[webpage]](https://robotic-view-transformer.github.io)
- **RVT-2: Learning Precise Manipulation from Few Examples**
  [[webpage]](https://robotic-view-transformer-2.github.io)
- **GR-1: Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation**
  [[webpage]](https://gr1-manipulation.github.io)
- **GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation**
  [[webpage]](https://gr2-manipulation.github.io)
- **ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation**
  [[webpage]](https://rekep-robot.github.io)
- **Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation**
  [[webpage]](https://homangab.github.io/gen2act)
- **OpenVLA: An Open-Source Vision-Language-Action Model**
  [[webpage]](https://openvla.github.io/)
- **RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation**
  [[webpage]](https://rdt-robotics.github.io/rdt-robotics)
- **π0: Our First Generalist Policy**
  [[webpage]](https://www.physicalintelligence.company/blog/pi0)  
    
