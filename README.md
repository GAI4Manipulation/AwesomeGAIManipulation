# AwesomeGAIManipulation

## Survey

## Data Generation

## Reward Generation
- **Language to Rewards for Robotic Skill Synthesis (CoRL 2023)**
  [[paper]](https://openreview.net/forum?id=SgTPdyehXMA)
  [[code]](https://github.com/google-deepmind/language_to_reward_2023)
  [[webpage]](https://language-to-reward.github.io/)
- **Vision-Language Models as Success Detectors (CoLLA 2023)**
  [[paper]](https://proceedings.mlr.press/v232/du23b/du23b.pdf)
- **Scaling robot policy learning via zero-shot labeling with foundation models (CoRL 2024)**
  [[paper]](https://arxiv.org/abs/2410.17772)
  [[code]](https://robottasklabeling.github.io/)
  [[webpage]](https://robottasklabeling.github.io/) 
- **FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning (ICML 2024)**
  [[paper]]([https://arxiv.org/abs/2402.00000](https://arxiv.org/abs/2406.00645))
  [[code]](https://github.com/fuyw/FuRL)  
- **Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning (ICLR 2024)**
  [[paper]](https://openreview.net/forum?id=tUM39YTRxH)
- **Eureka: Human-Level Reward Design via Coding Large Language Models (NeurIPS 2023)**
  [[paper]](https://arxiv.org/abs/2310.12931)
- **CLIPort: What and Where Pathways for Robotic Manipulation**
  [[paper]](https://arxiv.org/abs/2109.12098)
- **R3M: A Universal Visual Representation for Robot Manipulation**
  [[paper]](https://arxiv.org/abs/2203.12601)
  [[code]](https://github.com/facebookresearch/r3m)
  [[webpage]](https://sites.google.com/view/robot-r3m/?pli=1)
- **LIV: Language-Image Representations and Rewards for Robotic Control (ICML 2023)**
  [[paper]](https://arxiv.org/abs/2306.00958)
  [[code]](https://github.com/penn-pal-lab/LIV)
  [[webpage]](https://penn-pal-lab.github.io/LIV/)
- **Learning Reward Functions for Robotic Manipulation by Observing Humans**
  [[paper]](https://arxiv.org/abs/2211.09019)
- **Deep visual foresight for planning robot motion (ICRA 2017)**
  [[paper]](https://arxiv.org/abs/1610.00696)
- **VLMPC: Vision-Language Model Predictive Control for Robotic Manipulation (RSS 2024)**
  [[paper]](https://arxiv.org/abs/2407.09829)
  [[code]](https://github.com/PPjmchen/VLMPC)
- **Learning Reward for Robot Skills Using Large Language Models via Self-Alignment (ICML 2024)**
  [[paper]](https://arxiv.org/abs/2405.07162)
- **Video Prediction Models as Rewards for Reinforcement Learning**
  [[paper]](https://arxiv.org/abs/2305.14343)
  [[code]](https://escontrela.me/viper)
- **Vip: Towards universal visual reward and representation via value-implicit pre-training (ICLR 2023)**
  [[paper]](https://arxiv.org/abs/2210.00030)
  [[code]](https://github.com/facebookresearch/vip)
- **Learning to Understand Goal Specifications by Modelling Reward**
  [[paper]](https://arxiv.org/pdf/1806.01946)
- **Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks**
  [[paper]](https://arxiv.org/abs/2405.01534)
- **Policy improvement using language feedback models (NeurIPS 2024)**
  [[paper]](https://arxiv.org/abs/2402.07876)



## State Generation


- **Reinforcement learning with action-free pre-training from videos (ICML2022)**
  [[paper]](https://proceedings.mlr.press/v162/seo22a/seo22a.pdf)
  [[code]](https://github.com/younggyoseo/apv)
- **Mastering diverse domains through world models**
  [[paper]](https://arxiv.org/pdf/2301.04104v2)
  [[code]](https://github.com/danijar/dreamerv3)
  [[webpage]](https://danijar.com/project/dreamerv3/)
- **Dream to Control: Learning Behaviors by Latent Imagination**
  [[paper]](https://arxiv.org/abs/1912.01603)
- **Robot Shape and Location Retention in Video Generation Using Diffusion Models**
  [[paper]](https://arxiv.org/abs/2407.02873)
  [[code]](https://github.com/PengPaulWang/diffusion-robots)
- **Uncertainty-aware active learning of nerf-based object models for robot manipulators using visual and re-orientation actions**
  [[paper]](https://actnerf.github.io/)
  [[code]](https://github.com/ActNeRF/ActNeRF)
  [[webpage]](https://actnerf.github.io/)
- **Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors**
  [[paper]](https://arxiv.org/abs/2403.14526)
  [[code]](https://github.com/tsagkas/click2grasp)
  [[webpage]](https://tsagkas.github.io/click2grasp/)
- **Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL (ECCV2024)**
  [[paper]](https://arxiv.org/abs/2404.09857)
- **Doughnet: A visual predictive model for topological manipulation of deformable objects**
  [[paper]](https://arxiv.org/abs/2404.12524)
- **KISA: A Unified Keyframe Identifier and Skill Annotator for Long-Horizon Robotics Demonstrations (ICML2024)**
  [[paper]](https://openreview.net/pdf?id=oCI9gHocws)
- **DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems (ICML2024)**
  [[paper]](https://arxiv.org/abs/2407.11472)
- **Symmetry-Aware Robot Design with Structured Subgroups (ICML2023)**
  [[paper]](https://arxiv.org/abs/2306.00036)
- **Total-recon: Deformable scene reconstruction for embodied view synthesis (ICCV2023)**
  [[paper]](https://arxiv.org/abs/2304.12317)
  [[code & data]](https://github.com/andrewsonga/Total-Recon)
  [[webpage]](https://andrewsonga.github.io/totalrecon)
- **Explore and Tell: Embodied Visual Captioning in 3D Environments (ICCV2023)**
  [[paper]](https://arxiv.org/abs/2308.10447)
  [[code & data]](https://aim3-ruc.github.io/ExploreAndTell)
- **Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation (ECCV2024)**
  [[paper]](https://arxiv.org/abs/2405.01527)
- **Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation (CoRL2024)**
  [[paper]](https://arxiv.org/abs/2403.08321)
  [[code]](https://github.com/GuanxingLu/ManiGaussian)
  [[webpage]](https://guanxinglu.github.io/ManiGaussian/)
- **Physically Embodied Gaussian Splatting: A Realtime Correctable World Model for Robotics**
  [[paper]](https://arxiv.org/abs/2406.10788)
- **Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training (NeurIPS2024)**
  [[paper]](https://arxiv.org/pdf/2402.14407)
  [[code]](https://github.com/tinnerhrhe/VPDD)
  [[webpage]](https://video-diff.github.io/)
- **PreLAR: World Model Pre-training with Learnable Action Representation (ECCV2024)**
  [[paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03363.pdf)
  [[code]](https://github.com/zhanglixuan0720/PreLAR)
- **Octopus: Embodied vision-language programmer from environmental feedback**
  [[paper]](https://arxiv.org/abs/2310.08588)
  [[code]](https://github.com/dongyh20/Octopus)
  [[webpage]](https://choiszt.github.io/Octopus/)
- **Ec2: Emergent communication for embodied control (CVPR2023)**
  [[papar]](https://arxiv.org/abs/2304.09448)
- **Voxposer: Composable 3d value maps for robotic manipulation with language models**
  [[paper]](https://arxiv.org/abs/2307.05973)

## Language Generation
- **Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents (PMLR 2022)**
  [[paper]](https://arxiv.org/pdf/2201.07207.pdf)
  [[code]](https://github.com/huangwl18/language-planner)
- **Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition (PMLR 2023)**
  [[paper]](https://arxiv.org/abs/2307.14535)
  [[code]](https://github.com/real-stanford/scalingup)
- **Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks (ICLR 2024)**
  [[paper]](https://arxiv.org/pdf/2405.01534)
  [[code]](https://github.com/mihdalal/planseqlearn)
- **Large language models as commonsense knowledge for large-scale task planning (NeurIPS 2023)**
  [[paper]](https://arxiv.org/abs/2305.14078)
  [[code]](https://github.com/1989Ryan/llm-mcts)
- **REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction (CoRL 2023)**
  [[paper]](https://arxiv.org/abs/2306.15724)
  [[code]](https://github.com/real-stanford/reflect)
- **Gesture-Informed Robot Assistance via Foundation Models (CoRL 2023)**
  [[paper]](https://openreview.net/pdf?id=Ffn8Z4Q-zU)
- **Large Language Models for Robotics: Opportunities, Challenges, and Perspectives**
  [[paper]](https://arxiv.org/pdf/2401.04334)
- **Embodied Agent Interface (EAI): Benchmarking LLMs for Embodied Decision Making (NeurIPS 2024 Track Datasets and Benchmarks)**
  [[paper]](https://arxiv.org/abs/2410.07166)
  [[code]](https://github.com/embodied-agent-interface/embodied-agent-interface)
- **Embodiedgpt: Vision-language pre-training via embodied chain of thought (NeurIPS 2023)**
  [[paper]](https://arxiv.org/pdf/2305.15021.pdf)
  [[code]](https://github.com/OpenGVLab/EmbodiedGPT)
- **Chat with the Environment: Interactive Multimodal Perception using Large Language Models (IROS 2023)**
  [[paper]](https://arxiv.org/abs/2303.08268)
  [[code]](https://github.com/xf-zhao/Matcha)
- **Embodied CoT Distillation From LLM To Off-the-shelf Agents (ICML 2024)**
   [[paper]](https://arxiv.org/html/2412.11499v1)
- **Do as i can, not as i say: Grounding language in robotic affordances**
  [[paper]](https://say-can.github.io/assets/palm_saycan.pdf)
  [[code]](https://github.com/google-research/google-research/tree/master/saycan)
- **Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents (NeurIPS 2023)**
  [[paper]](https://openreview.net/pdf?id=JCCi58IUsh)
- **Inner Monologue: Embodied Reasoning through Planning with Language Models (CoRL 2022)**
 [[paper]](https://arxiv.org/abs/2207.05608)
- **PhyGrasp: Generalizing Robotic Grasping with Physics-informed Large Multimodal Models**
  [[paper]](https://arxiv.org/pdf/2402.16836.pdf)
  [[code]](https://github.com/dkguo/PhyGrasp)
- **SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning (CoRL 2023)**
  [[paper]](https://arxiv.org/abs/2307.06135)
- **Robomp2: A robotic multimodal perception-planning framework with multimodal large language models (ICML 2024)**
  [[paper]](https://arxiv.org/abs/2404.04929)
  [[code]](https://github.com/aopolin-lv/RoboMP2)
- **Text2Motion: From Natural Language Instructions to Feasible Plans (Autonomous Robots 2023)**
   [[paper]](https://openreview.net/pdf?id=M1yTyG5P7Cl)
- **STAP: Sequencing Task-Agnostic Policies (ICRA 2023)**
  [[paper]](https://arxiv.org/abs/2210.12250)
  [[code]](https://github.com/agiachris/STAP)
  
## Code Generation

## Visual Generation



## Grasp Generation

## Trajectory Generation
- **Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation**
  [[webpage]](https://mobile-aloha.github.io)
- **Diffusion Policy: Visuomotor Policy Learning via Action Diffusion**
  [[webpage]](https://diffusion-policy.cs.columbia.edu)
- **3D Diffuser Actor: Policy Diffusion with 3D Scene Representations**
  [[webpage]](https://3d-diffuser-actor.github.io)
- **RT-1: Robotics Transformer for Real-World Control at Scale**
  [[webpage]](https://robotics-transformer1.github.io)
- **RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control**
  [[webpage]](https://robotics-transformer2.github.io)
- **RVT: Robotic View Transformer for 3D Object Manipulation**
  [[webpage]](https://robotic-view-transformer.github.io)
- **RVT-2: Learning Precise Manipulation from Few Examples**
  [[webpage]](https://robotic-view-transformer-2.github.io)
- **GR-1: Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation**
  [[webpage]](https://gr1-manipulation.github.io)
- **GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation**
  [[webpage]](https://gr2-manipulation.github.io)
- **ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation**
  [[webpage]](https://rekep-robot.github.io)
- **Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation**
  [[webpage]](https://homangab.github.io/gen2act)
- **OpenVLA: An Open-Source Vision-Language-Action Model**
  [[webpage]](https://openvla.github.io/)
- **RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation**
  [[webpage]](https://rdt-robotics.github.io/rdt-robotics)
- **π0: Our First Generalist Policy**
  [[webpage]](https://www.physicalintelligence.company/blog/pi0)  
    
