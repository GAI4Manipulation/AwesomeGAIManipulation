# AwesomeGAIManipulation

## Survey

## Data Generation

## Reward Generation
- **Language to Rewards for Robotic Skill Synthesis (CoRL 2023)**
  [[paper]](https://openreview.net/forum?id=SgTPdyehXMA)
  [[code]](https://github.com/google-deepmind/language_to_reward_2023)
  [[webpage]](https://language-to-reward.github.io/)
- **Vision-Language Models as Success Detectors (CoLLA 2023)**
  [[paper]](https://proceedings.mlr.press/v232/du23b/du23b.pdf)
- **Scaling robot policy learning via zero-shot labeling with foundation models (CoRL 2024)**
  [[paper]](https://arxiv.org/abs/2410.17772)
  [[code]](https://robottasklabeling.github.io/)
  [[webpage]](https://robottasklabeling.github.io/) 
- **FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning (ICML 2024)**
  [[paper]]([https://arxiv.org/abs/2402.00000](https://arxiv.org/abs/2406.00645))
  [[code]](https://github.com/fuyw/FuRL)  
- **Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning (ICLR 2024)**
  [[paper]](https://openreview.net/forum?id=tUM39YTRxH)
- **Eureka: Human-Level Reward Design via Coding Large Language Models (NeurIPS 2023)**
  [[paper]](https://arxiv.org/abs/2310.12931)
- **CLIPort: What and Where Pathways for Robotic Manipulation**
  [[paper]](https://arxiv.org/abs/2109.12098)
- **R3M: A Universal Visual Representation for Robot Manipulation**
  [[paper]](https://arxiv.org/abs/2203.12601)
  [[code]](https://github.com/facebookresearch/r3m)
  [[webpage]](https://sites.google.com/view/robot-r3m/?pli=1)
- **LIV: Language-Image Representations and Rewards for Robotic Control (ICML 2023)**
  [[paper]](https://arxiv.org/abs/2306.00958)
  [[code]](https://github.com/penn-pal-lab/LIV)
  [[webpage]](https://penn-pal-lab.github.io/LIV/)
  Learning Reward Functions for Robotic Manipulation by Observing Humans
- **Deep visual foresight for planning robot motion (ICRA 2017)**
  [[paper]](https://arxiv.org/abs/1610.00696)
- **VLMPC: Vision-Language Model Predictive Control for Robotic Manipulation (RSS 2024)**
  [[paper]](https://arxiv.org/abs/2407.09829)
  [[code]](https://github.com/PPjmchen/VLMPC)
- **Learning Reward for Robot Skills Using Large Language Models via Self-Alignment (ICML 2024)**
  [[paper]](https://arxiv.org/abs/2405.07162)
- **Video Prediction Models as Rewards for Reinforcement Learning**
  [[paper]](https://arxiv.org/abs/2305.14343)
  [[code]](https://escontrela.me/viper)
- **Vip: Towards universal visual reward and representation via value-implicit pre-training (ICLR 2023)**
  [[paper]](https://arxiv.org/abs/2210.00030)
  [[code]](https://github.com/facebookresearch/vip)
- **Learning to Understand Goal Specifications by Modelling Reward**
  [[paper]](https://arxiv.org/pdf/1806.01946)
- **Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks**
  [[paper]](https://arxiv.org/abs/2405.01534)
- **Policy improvement using language feedback models (NeurIPS 2024)**
  [[paper]](https://arxiv.org/abs/2402.07876)

## Image Generation

## State Generation

## Language Generation

## Grasp Generation

## Trajectory Generation
