# AwesomeGAIManipulation

## Survey

## Data Generation

## Reward Generation
- **Language to Rewards for Robotic Skill Synthesis (CoRL 2023)**
  [[paper]](https://openreview.net/forum?id=SgTPdyehXMA)
  [[code]](https://github.com/google-deepmind/language_to_reward_2023)
  [[webpage]](https://language-to-reward.github.io/)
- **Vision-Language Models as Success Detectors (CoLLA 2023)**
  [[paper]](https://proceedings.mlr.press/v232/du23b/du23b.pdf)
- **Scaling robot policy learning via zero-shot labeling with foundation models (CoRL 2024)**
  [[paper]](https://arxiv.org/abs/2410.17772)
  [[code]](https://robottasklabeling.github.io/)
  [[webpage]](https://robottasklabeling.github.io/) 
- **FuRL: Visual-Language Models as Fuzzy Rewards for Reinforcement Learning (ICML 2024)**
  [[paper]]([https://arxiv.org/abs/2402.00000](https://arxiv.org/abs/2406.00645))
  [[code]](https://github.com/fuyw/FuRL)  
- **Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning (ICLR 2024)**
  [[paper]](https://openreview.net/forum?id=tUM39YTRxH)
- **Eureka: Human-Level Reward Design via Coding Large Language Models (NeurIPS 2023)**
  [[paper]](https://arxiv.org/abs/2310.12931)
- **Agentic Skill Discovery (CoRL 2024 workshop & ICRA@40)**
  [[paper]](https://arxiv.org/abs/2405.15019)
  [[code]](https://github.com/xf-zhao/Agentic-Skill-Discovery)
- **CLIPort: What and Where Pathways for Robotic Manipulation**
  [[paper]](https://arxiv.org/abs/2109.12098)
- **R3M: A Universal Visual Representation for Robot Manipulation**
  [[paper]](https://arxiv.org/abs/2203.12601)
  [[code]](https://github.com/facebookresearch/r3m)
  [[webpage]](https://sites.google.com/view/robot-r3m/?pli=1)
- **LIV: Language-Image Representations and Rewards for Robotic Control (ICML 2023)**
  [[paper]](https://arxiv.org/abs/2306.00958)
  [[code]](https://github.com/penn-pal-lab/LIV)
  [[webpage]](https://penn-pal-lab.github.io/LIV/)
- **Learning Reward Functions for Robotic Manipulation by Observing Humans**
  [[paper]](https://arxiv.org/abs/2211.09019)
- **Deep visual foresight for planning robot motion (ICRA 2017)**
  [[paper]](https://arxiv.org/abs/1610.00696)
- **VLMPC: Vision-Language Model Predictive Control for Robotic Manipulation (RSS 2024)**
  [[paper]](https://arxiv.org/abs/2407.09829)
  [[code]](https://github.com/PPjmchen/VLMPC)
- **Learning Reward for Robot Skills Using Large Language Models via Self-Alignment (ICML 2024)**
  [[paper]](https://arxiv.org/abs/2405.07162)
- **Video Prediction Models as Rewards for Reinforcement Learning**
  [[paper]](https://arxiv.org/abs/2305.14343)
  [[code]](https://escontrela.me/viper)
- **Vip: Towards universal visual reward and representation via value-implicit pre-training (ICLR 2023)**
  [[paper]](https://arxiv.org/abs/2210.00030)
  [[code]](https://github.com/facebookresearch/vip)
- **Learning to Understand Goal Specifications by Modelling Reward**
  [[paper]](https://arxiv.org/pdf/1806.01946)
- **Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks**
  [[paper]](https://arxiv.org/abs/2405.01534)
- **Policy improvement using language feedback models (NeurIPS 2024)**
  [[paper]](https://arxiv.org/abs/2402.07876)

## Image Generation

## State Generation


- **Reinforcement learning with action-free pre-training from videos (ICML2022)**
  [[paper]](https://proceedings.mlr.press/v162/seo22a/seo22a.pdf)
  [[code]](https://github.com/younggyoseo/apv)
- **Mastering diverse domains through world models**
  [[paper]](https://arxiv.org/pdf/2301.04104v2)
  [[code]](https://github.com/danijar/dreamerv3)
  [[webpage]](https://danijar.com/project/dreamerv3/)
- **Dream to Control: Learning Behaviors by Latent Imagination**
  [[paper]](https://arxiv.org/abs/1912.01603)
- **Robot Shape and Location Retention in Video Generation Using Diffusion Models**
  [[paper]](https://arxiv.org/abs/2407.02873)
  [[code]](https://github.com/PengPaulWang/diffusion-robots)
- **Uncertainty-aware active learning of nerf-based object models for robot manipulators using visual and re-orientation actions**
  [[paper]](https://actnerf.github.io/)
  [[code]](https://github.com/ActNeRF/ActNeRF)
  [[webpage]](https://actnerf.github.io/)
- **Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors**
  [[paper]](https://arxiv.org/abs/2403.14526)
  [[code]](https://github.com/tsagkas/click2grasp)
  [[webpage]](https://tsagkas.github.io/click2grasp/)
- **Empowering Embodied Visual Tracking with Visual Foundation Models and Offline RL (ECCV2024)**
  [[paper]](https://arxiv.org/abs/2404.09857)
- **Doughnet: A visual predictive model for topological manipulation of deformable objects**
  [[paper]](https://arxiv.org/abs/2404.12524)
- **KISA: A Unified Keyframe Identifier and Skill Annotator for Long-Horizon Robotics Demonstrations (ICML2024)**
  [[paper]](https://openreview.net/pdf?id=oCI9gHocws)
- **DynSyn: Dynamical Synergistic Representation for Efficient Learning and Control in Overactuated Embodied Systems (ICML2024)**
  [[paper]](https://arxiv.org/abs/2407.11472)
- **Symmetry-Aware Robot Design with Structured Subgroups (ICML2023)**
  [[paper]](https://arxiv.org/abs/2306.00036)
- **Total-recon: Deformable scene reconstruction for embodied view synthesis (ICCV2023)**
  [[paper]](https://arxiv.org/abs/2304.12317)
  [[code & data]](https://github.com/andrewsonga/Total-Recon)
  [[webpage]](https://andrewsonga.github.io/totalrecon)
- **Explore and Tell: Embodied Visual Captioning in 3D Environments (ICCV2023)**
  [[paper]](https://arxiv.org/abs/2308.10447)
  [[code & data]](https://aim3-ruc.github.io/ExploreAndTell)
- **Track2Act: Predicting Point Tracks from Internet Videos enables Generalizable Robot Manipulation (ECCV2024)**
  [[paper]](https://arxiv.org/abs/2405.01527)
- **Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation (CoRL2024)**
  [[paper]](https://arxiv.org/abs/2403.08321)
  [[code]](https://github.com/GuanxingLu/ManiGaussian)
  [[webpage]](https://guanxinglu.github.io/ManiGaussian/)
- **Physically Embodied Gaussian Splatting: A Realtime Correctable World Model for Robotics**
  [[paper]](https://arxiv.org/abs/2406.10788)
- **Learning an Actionable Discrete Diffusion Policy via Large-Scale Actionless Video Pre-Training (NeurIPS2024)**
  [[paper]](https://arxiv.org/pdf/2402.14407)
  [[code]](https://github.com/tinnerhrhe/VPDD)
  [[webpage]](https://video-diff.github.io/)
- **PreLAR: World Model Pre-training with Learnable Action Representation (ECCV2024)**
  [[paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03363.pdf)
  [[code]](https://github.com/zhanglixuan0720/PreLAR)
- **Octopus: Embodied vision-language programmer from environmental feedback**
  [[paper]](https://arxiv.org/abs/2310.08588)
  [[code]](https://github.com/dongyh20/Octopus)
  [[webpage]](https://choiszt.github.io/Octopus/)
- **Ec2: Emergent communication for embodied control (CVPR2023)**
  [[papar]](https://arxiv.org/abs/2304.09448)
- **Voxposer: Composable 3d value maps for robotic manipulation with language models**
  [[paper]](https://arxiv.org/abs/2307.05973)

## Language Generation

## Grasp Generation

## Trajectory Generation
